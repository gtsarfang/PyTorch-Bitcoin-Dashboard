import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler

dataframe = pd.read_csv('coinbase_data_clean.csv')
data = dataframe.drop(['Timestamp'],axis=1)
output = data['WP_Increase']
data.drop(labels=['WP_Increase'],axis=1,inplace=True)
data.insert(len(data.columns),'WP_Increase',output)

# Need to drop timestamp to be able to convert to tensor
X = data.iloc[:,0:-1].values
y = data.iloc[:,-1].values

from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=0)

sc = MinMaxScaler()
sc = sc.fit(X_train)
X_train = sc.transform(X_train)
X_test = sc.transform(X_test)

y_test

### Create Tensors

# Pytorch Libraries
import torch
import torch.nn as nn # helps create models
import torch.nn.functional as F

# Convert arrays to tensors for Pytorch to process
X_train = torch.FloatTensor(X_train)
X_test = torch.FloatTensor(X_test)
y_train = torch.LongTensor(y_train)
y_test = torch.LongTensor(y_test)

# Create Model
class ANN(nn.Module):
    
    def __init__(self,input_features=16,hidden1=20,hidden2=20,out_features=2):
        super().__init__()
        self.f_connected1 = nn.Linear(input_features,hidden1) # connect input to hidden layer 1
        self.f_connected2 = nn.Linear(hidden1,hidden2) # connect hidden layer 1 to hiddent layer 2
        self.out = nn.Linear(hidden2,out_features) # connect hidden 2 to output
        
    def forward(self,x): # forward propogation
            x = F.relu(self.f_connected1(x)) 
            x = F.relu(self.f_connected2(x))
            x = self.out(x)
            
            return x

### Instantialize ANN Model

torch.manual_seed(20)
model = ANN()

model.parameters

### Backpropogation

# Define loss function, define optimizer
loss_function = nn.CrossEntropyLoss() # useful for multiclassification problem
optimizer = torch.optim.Adam(model.parameters(),lr=0.01)

epochs=500
final_losses = []
for i in range(epochs):
    i += 1
    y_pred = model.forward(X_train)
    loss = loss_function(y_pred,y_train)
    final_losses.append(loss)
    if i%10 == 1:
        print('Epoch number:{} and the loss: {}'.format(i,loss.item()))
    
    # optimizer
    optimizer.zero_grad() # help reduce loss
    loss.backward() # backpropogate - find derivative
    optimizer.step() # performs single optimization step

 # plot loss function
plt.plot(range(epochs),final_losses)
plt.ylabel('Loss')
plt.xlabel('Epoch')

### Deploy model on test data

pred = []

with torch.no_grad():
    for i,data in enumerate(X_test):
        print(model(data))

# predict classifcation - to buy or not to buy
pred2 = []

with torch.no_grad():
    for i,data in enumerate(X_test):
        pred = model(data)
        pred2.append(pred.argmax().item())
        print(pred.argmax().item()) # argmax returns index with maximum value; item returns 0 or 1

from sklearn.metrics import confusion_matrix,classification_report,accuracy_score
cm = confusion_matrix(y_test,pred2)
print(confusion_matrix(y_test,pred2))
print(classification_report(y_test,pred2))
print(accuracy_score(y_test,pred2))

